# -*- coding: utf-8 -*-
"""LSTM_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OzmR-fMIMidSdsLUb1P1_fFajY7pRjNv

# Initialize runtime
"""

import tensorflow as tf
print('***** TENSORFLOW version', tf.__version__)

if tf.__version__ == '1.15.0':
  print('Installing latest version of TF', tf.__version__)
  !pip uninstall tensorflow -y
  !pip install tensorflow-gpu
!pip3 install sklearn
!pip3 install pandas

"""# Print runtime"""

import tensorflow as tf
print(tf.__version__)

"""# Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Load data, Process data"""

import os
import numpy as np
import tensorflow as tf
import sys
# -------------- for lstm
def load_X(path):
  # try:
  the_file = None
  X_signals = []
  files = os.listdir(path)
  files.sort(key=str.lower)
  for my_file in files:
      the_file = my_file
      fileName = os.path.join(path, my_file)
      file = open(fileName, 'r')
      X_signals.append(
          [np.array(cell, dtype=np.float32) for cell in [
              row.strip().split(' ') for row in file
          ]]
      )
      file.close()
      # X_signals = 6*totalStepNum*128
  return np.transpose(np.array(X_signals), (1, 2, 0)) 
  # except:
  #   print(the_file)
  #   print(np.array(X_signals).shape)
  #   sys.exit()
    #                               0              1    2   
    # lstm input shape = (total_num_of_trials x 128 x 6)


def load_y(y_path):
    file = open(y_path, 'r')
    # Read dataset from disk, dealing with text file's syntax
    y_ = np.array(
        [elem for elem in [
            row.replace('  ', ' ').strip().split(' ') for row in file
        ]],
        dtype=np.int32
    )
    file.close()
    # Substract 1 to each output class for friendly 0-based indexing
    y_ = y_ - 1
    # one_hot
    y_ = y_.reshape(len(y_))
    n_values = int(np.max(y_)) + 1
    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS

def load_dataset(path):
    X_signals = []
    files = os.listdir(path)
    files.sort(key=str.lower)
    for my_file in files:
        fileName = os.path.join(path, my_file)
        file = open(fileName, 'r')
        X_signals.append(
            [np.array(cell, dtype=np.float32) for cell in [
                row.strip().split(' ') for row in file
            ]]
        )
        file.close()
    return np.transpose(np.array(X_signals), (1, 2, 0)) 

def load_labels(path):
    file = open(path, 'r')
    # Read dataset from disk, dealing with text file's syntax
    y_ = np.array(
        [elem for elem in [
            row.replace('  ', ' ').strip().split(' ') for row in file
        ]],
        dtype=np.int32
    )
    file.close()
    # Substract 1 to each output class for friendly 0-based indexing
    y_ = y_ - 1
    # one_hot
    y_ = y_.reshape(len(y_))
    n_values = int(np.max(y_)) + 1
    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS

### new way of loading data; this will replace the existing way
def load_data(train_split=0.8, validate_split=0.0, test_split=0.2):
  dataset = load_dataset(path + '/train')
  labels = load_labels(path + '/y_train.txt')

  n = len(dataset)

  print('Total samples: ', len(labels), '\n')

  # split data into training, validation and testing
  x_train = dataset[:int(n*train_split)]
  x_validate = dataset[-int(n*validate_split):]
  x_test = dataset[-int(n*test_split):]

  # split labels into training, validation and testing
  y_train = labels[:int(n*train_split)]
  y_validate = labels[-int(n*validate_split):]
  y_test = labels[-int(n*test_split):]

  return (x_train, y_train), (x_validate, y_validate), (x_test, y_test)

legacy = True

if legacy:
  # path = '/content/drive/My Drive/dataset/' + 'Dataset#1'
  
  # path = '/content/drive/My Drive/eecs4088/zou/dataset/Dataset#2'
  # path = '/content/drive/My Drive/eecs4088/zou/dataset/Dataset#1'
  # path = '/content/drive/My Drive/eecs4088/zou/dataset/d1_4unique_4overlap'
  # path = '/content/drive/My Drive/eecs4088/zou/dataset/d1_1unique_0overlap'
  # path = '/content/drive/My Drive/eecs4088/zou/dataset/half_unique'
  # path = '/content/drive/My Drive/Gait Identification and Analysis/datasets/human-gait-db/89-subjects-one-session-d1'

  # path = '/content/drive/My Drive/eecs4088/zou/dataset/Dataset#3'
  path = '/content/drive/My Drive/eecs4088/zou/dataset/human-gait-db/89-subjects-one-session-d1'
  X = load_X(path + '/imu')
  # X_test = load_X(path + '/test/InertialSignals')
  y = load_y(path + '/labels/train_id.txt')
  # test_label = load_y(path + '/test/y_test.txt')

  # X_train = load_X(path + '/train/InertialSignals')
  # X_test = load_X(path + '/test/InertialSignals')
  # train_label = load_y(path + '/train/y_train.txt')
  # test_label = load_y(path + '/test/y_test.txt')
  number_of_subjects = 118
else:
  path = '/content/drive/My Drive/eecs4088/zou/dataset/Dataset#1'
  number_of_subjects = 118
  (X_train, train_label), (X_validate, validate_label), (X_test, test_label) = load_data(train_split=0.8, validate_split=0.1, test_split=0.1)

print('X_train.shape : ', X_train.shape, '\n')
# print('X_validate.shape : ', X_validate.shape, '\n')
print('X_test.shape : ', X_test.shape, '\n')

"""split proeprly"""

from sklearn.model_selection import train_test_split
# X = np.concatenate((X_train, X_test), axis=0)
# display(X.shape)
# y = np.concatenate((train_label, test_label), axis=0)

X_train, X_test, train_label, test_label = train_test_split(X, y, test_size=0.1, random_state=42)

"""# Check data"""

# X_train = X_train[:-104]
# train_label = train_label[:-104]

# X_test = X_test[:-240]
# test_label = test_label[:-240]
import numpy as np
print('X_train.shape : ', X_train.shape)
print('len(train_label) : ', len(train_label))
print('X_test.shape : ', X_test.shape)
print('len(test_label) : ', len(test_label))
print(number_of_subjects)
print(np.unique(train_label).shape)

print((np.unique(test_label.squeeze().argmax(axis=1))).shape)

# todo: check data points per subject are roughly equivalent

"""[link text](https://)Resampling imbalanced data"""

# Technique # 1: Under-sample based on randomly picking samples with or without replacement
# Docs: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(sampling_strategy='majority', random_state=None, replacement=False)

from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy='minority', random_state=None, k_neighbors=5)
from imblearn.combine import SMOTEENN
sme = SMOTEENN(sampling_strategy='all', random_state=None, smote=smote, enn=None)

dataPoints = 64
import copy
arr = copy.deepcopy(X_train)
print('train_label', train_label.shape)
orig_shape = arr.shape
print('orig shape', orig_shape)

print((arr.shape[1]*arr.shape[2] ))


arr = np.reshape(arr, (arr.shape[0], (arr.shape[1]*arr.shape[2] )))
print('reshaped', arr.shape)

arr, arr_2 = smote.fit_sample(arr, train_label)

print('resampled train', arr.shape)
print('resampled train_label', arr_2.shape)

arr = np.reshape(arr, (-1,dataPoints,6))
print('undo to orig shape', arr.shape)

X_train = arr
train_label = arr_2
# X_train = arr



"""Butterworth Noise Filter"""

from scipy.signal import butter, lfilter
import numpy as np 
import numpy as np
from scipy.signal import freqz
    
def butter_bandpass(lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = lfilter(b, a, data)
    return y

def butter_worth_main(data):
    # Sample rate and desired cutoff frequencies (in Hz).
    fs = 64.0
    lowcut = 6
    highcut = 16
    # Filter a noisy signal.
    y = butter_bandpass_filter(data, lowcut, highcut, fs, order=1)
    return y

# bw_X_train = np.apply_along_axis(butter_worth_main, 1, X_train)
bw_X_train = np.apply_along_axis(butter_worth_main, 1, X_train)
bw_X_test = np.apply_along_axis(butter_worth_main, 1, X_test)

# display(bw_X_test.shape)

"""Data Normalization"""

nm_bw_X_train = tf.keras.utils.normalize(
    bw_X_train, axis=-1, order=1
)

nm_bw_X_test = tf.keras.utils.normalize(
    bw_X_test, axis=-1, order=2
)
display(train_label.shape)
display(nm_bw_X_train.shape)

display(test_label.shape)
display(nm_bw_X_test.shape)
# display(nm_bw_X_test)

display(X_test)
display(bw_X_test)

"""# reshape data"""

X_test = X_test.reshape(-1,6,128,1)
X_test.shape
# print(X_test.ndim, X_test.shape)

# X_train.take(0)
# X_train.shape
# X_train[0][0]


# X_test = np.transpose(np.array(X_test), (1, 0, 2))#(totalStepNum*6*128)
# print(X_test.ndim, X_test.shape)

# 0    1     2
# (6, 3740, 128)
# (3740, 6, 128) for each trial (for each acc/gyr_p (for each datapoint))

"""# LSTM"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Flatten, Dense, Dropout
from tensorflow.keras.layers import StackedRNNCells
from tensorflow.keras.layers import LSTMCell
from numpy import array

from tensorflow.keras.layers import TimeDistributed
from tensorflow.keras.layers import Bidirectional

from tensorflow.keras.layers import Conv1D, MaxPooling1D
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Dense, Dropout, Activation

n_inputs = len(X_train[0][0]) #6
n_hidden = 64
n_steps = len(X_train[0]) #128
n_features = len(X_train[0][0])
learning_rate = 0.0025
t_batch_size = 1500

batch_size = 512
no_epochs = 100 # 200
verbosity = 1

lstm_model = Sequential()
classes = 90

lstm_model.add(LSTM(128,input_shape=(n_steps, n_features), return_sequences=True))
lstm_model.add(LSTM(64))


lstm_model.add(Dropout(0.50))
lstm_model.add(Flatten())
lstm_model.add(Dense(classes, activation='softmax'))


# Compile the model
lstm_model.compile(loss='categorical_crossentropy',
              optimizer='RMSProp',
              metrics=['accuracy'])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import plot_model
plot_model(lstm_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

"""# Model Summary"""

# display(X_train.shape)
# lstm_model.build( X_train.shape)
lstm_model.summary()
lstm_model.get_config()

"""# Check accuracy"""

history = lstm_model.fit(X_train, train_label,
          batch_size=batch_size,
          epochs=no_epochs,
          validation_data=(X_test, test_label),
          shuffle=True,
          verbose=verbosity)

test_loss, test_acc = lstm_model.evaluate(X_test, test_label, verbose=2)

print('\nTest accuracy:', test_acc)

history = lstm_model.fit(X_train, train_label,
          batch_size=batch_size,
          epochs=no_epochs,
          validation_data=(X_test, test_label),
          shuffle=True,
          verbose=verbosity)

test_loss, test_acc = lstm_model.evaluate(X_test, test_label, verbose=2)
print('\nTest accuracy:', test_acc)

"""# Plot loss/epoch"""

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.title('loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['loss'], loc='upper left')
plt.show()

Find wrongly predicted classes

import sys
# import numpy
# numpy.set_printoptions(threshold=1000)

y_pred1 = lstm_model.predict(X_test)
pred_arr = np.argmax(y_pred1,axis=1)
print(pred_arr.shape)
one_hot_targets = np.eye(118)[pred_arr]
print(one_hot_targets.shape)
print(test_label.shape)
# print(pred_arr.reshape((-1,)).shape)
# print(pred_arr.reshape((-1,)))
# display(test_label.reshape((-1,)))
print(one_hot_targets)
incorrects = np.nonzero(one_hot_targets != test_label)
print(incorrects)
wrongly_pred_samples, classes = incorrects
print('no. of samples wrongly pred', len(wrongly_pred_samples))
print(sorted(set(classes)))
print(len(sorted(set(classes))))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import plot_model

plot_model(lstm_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

from tensorflow.keras import backend as K
K.clear_session()

